{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaygopalkr/EM-VLM4AD/blob/main/eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g9Mvah--8P0"
      },
      "source": [
        "## Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgN9hDNg98Ia"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install pycocoevalcap\n",
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFf7kqkFbi8B"
      },
      "source": [
        "Remember to comment out evaluating SPICe by commenting it out in /usr/local/lib/python3.10/dist-packages/pycocoevalcap/eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEsFA6PcHkJG",
        "outputId": "4a5bb789-a12b-4bcc-fabf-8b2eed606608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvpVEJa3-NQi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "import os\n",
        "from collections import namedtuple\n",
        "from tqdm import tqdm as progress_bar\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model, LoftQConfig\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vit_b_32\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3HqqZ1F_E8d"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh73Yohr_J3W"
      },
      "outputs": [],
      "source": [
        "!unzip -q drive/MyDrive/DriveLM/data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVUAX8Pg-_et"
      },
      "source": [
        "## Dataset Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DUN5Xq4-QPr"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiFrameDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_file, tokenizer, transform=None):\n",
        "        with open(input_file) as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the question and answer at the idx\n",
        "        qa, img_path = self.data[idx]\n",
        "        img_paths = list(img_path.values())\n",
        "\n",
        "        q_text, a_text = qa['Q'], qa['A']\n",
        "        q_text = f\"Question: {q_text} Answer:\"\n",
        "\n",
        "        # Concatenate images into a single tensor\n",
        "        imgs = [self.transform(read_image(p).float()).to(device) for p in img_paths]\n",
        "        imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "        return q_text, imgs, a_text, sorted(list(img_path.values()))\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        q_texts, imgs, a_texts, _ = zip(*batch)\n",
        "        imgs = torch.stack(list(imgs), dim=0)\n",
        "\n",
        "        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "        return encodings, imgs, labels\n",
        "\n",
        "    def collate_fn_test(self, batch):\n",
        "\n",
        "        q_texts, imgs, a_texts, img_paths = zip(*batch)\n",
        "\n",
        "        imgs = torch.stack(list(imgs), dim=0)\n",
        "        img_paths = list(img_paths)\n",
        "        encodings = self.tokenizer(q_texts, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        labels = self.tokenizer(a_texts, padding=True, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "        return q_texts, encodings, imgs, labels, img_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71LVhAHBBGCw"
      },
      "source": [
        "## Model Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JQBkN3gBG2f"
      },
      "outputs": [],
      "source": [
        "VIT_HIDDEN_STATE = 768\n",
        "VIT_SEQ_LENGTH = 49\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class DriveVLMT5(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Make tokenizer and text model\n",
        "        if config.lm == 'T5-Base':\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-base')\n",
        "        else:\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained('google-t5/t5-large')\n",
        "\n",
        "            # For quantization\n",
        "            loftq_config = LoftQConfig(loftq_bits=8)\n",
        "            # Create LoRA model\n",
        "            lora_config = LoraConfig(\n",
        "                r=64,\n",
        "                lora_alpha=32,\n",
        "                loftq_config=loftq_config,\n",
        "                lora_dropout=0.05,\n",
        "                bias='none',\n",
        "                target_modules=['q', 'v']\n",
        "            )\n",
        "            self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        hidden_size = self.model.config.d_model\n",
        "\n",
        "        print('Trainable Parameters for LM model:')\n",
        "        print_trainable_parameters(self.model)\n",
        "\n",
        "        # Create instance for multi-view processor\n",
        "        self.mvp = self.MultiViewProcessor(config.gpa_hidden_size, hidden_size, config.lm, freeze=True)\n",
        "\n",
        "    class MultiViewProcessor(nn.Module):\n",
        "\n",
        "        def __init__(self, gpa_hidden_size, hidden_size, lm, freeze=False):\n",
        "\n",
        "            super().__init__()\n",
        "\n",
        "            # Use ViT for image embeddings\n",
        "            self.img_model = vit_b_32(weights='DEFAULT')\n",
        "            self.lm = lm\n",
        "\n",
        "            # Modal embedding to distinguish between image and text\n",
        "            self.modal_embeddings = nn.Embedding(2, hidden_size)\n",
        "            self.modal_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "            # If we are freezing the CLIP embeddings\n",
        "            if freeze:\n",
        "                for param in self.img_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            # Set matrices based on MIVC paper\n",
        "            self.w = nn.Linear(in_features=gpa_hidden_size, out_features=1)\n",
        "            self.Z = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "            self.G = nn.Sequential(\n",
        "                nn.Linear(in_features=VIT_HIDDEN_STATE * VIT_SEQ_LENGTH, out_features=gpa_hidden_size, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "            if self.lm != 'T5-Base':\n",
        "              self.img_projection_layer = nn.Linear(in_features=VIT_HIDDEN_STATE, out_features=hidden_size)\n",
        "\n",
        "\n",
        "        def gpa(self, img_embeddings):\n",
        "\n",
        "            \"\"\"\"\n",
        "            Calculates the gated-pooling attention score for the image embeddings\n",
        "            :param img_embeddings: (6x768) dimensional\n",
        "            :return single embedding of size (768,)\n",
        "            \"\"\"\n",
        "\n",
        "            # Get weights for gated pooling attention\n",
        "            gpa_weights = torch.softmax(self.w(self.Z(img_embeddings) * self.G(img_embeddings)), dim=0  )\n",
        "\n",
        "            # Take a linear combination of all the image embeddings\n",
        "            fused_embeddings = torch.sum(gpa_weights * img_embeddings, dim=0)\n",
        "\n",
        "            return fused_embeddings\n",
        "\n",
        "        def get_img_embedding(self, imgs):\n",
        "\n",
        "            N = imgs.shape[0]\n",
        "\n",
        "            # Process into patches (N x 6 x 49 x H)\n",
        "            merged_embedding = torch.stack([self.img_model._process_input(img) for img in imgs], dim=0)\n",
        "\n",
        "            # Concatenate the batch class tokens -> (N, 6, 50, H)\n",
        "            batch_class_tokens = self.img_model.class_token.expand(merged_embedding.shape[1], -1, -1).repeat(N, 1, 1, 1)\n",
        "            merged_embedding = torch.cat([batch_class_tokens, merged_embedding], dim=2)\n",
        "\n",
        "            # Add positional embeddings and remove class token -> (N, 6, 49, H)\n",
        "            merged_embedding += self.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)\n",
        "            merged_embedding = merged_embedding[:, :, 1:]\n",
        "\n",
        "            # Get merged embedding and reshape to 2D embedding -> (N, 1, 49, H)\n",
        "            merged_embedding = torch.stack([self.gpa(embedding.flatten(start_dim=1)).reshape(VIT_SEQ_LENGTH,\n",
        "                                            VIT_HIDDEN_STATE) for embedding in merged_embedding], dim=0)\n",
        "\n",
        "            # Project to VL dimension -> (1, 49, H) (H is 512 for t5-small, 768 for t5-base)\n",
        "            if self.lm != 'T5-Base':\n",
        "              merged_embedding = self.img_projection_layer(merged_embedding)\n",
        "\n",
        "            # Add modal type embedding to merged embedding\n",
        "            merged_embedding += self.modal_embeddings(\n",
        "                torch.ones((1, merged_embedding.shape[1]), dtype=torch.int, device=device))\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "        def forward(self, text_enc, imgs, text_model):\n",
        "\n",
        "            # Get the image embeddings (N x 1 x 49 x H)\n",
        "            imgs_embedding = self.get_img_embedding(imgs)\n",
        "\n",
        "            # Get the text embeddings (N x S x H)\n",
        "            text_embeddings = text_model.get_input_embeddings()(text_enc)\n",
        "\n",
        "            # Add modal embeddings to text\n",
        "            text_embeddings += self.modal_embeddings(torch.zeros((1, text_embeddings.shape[1]), dtype=torch.int,\n",
        "                                                                 device=device))\n",
        "\n",
        "            # Concatenate embeddings -> (1 x S x 512)\n",
        "            merged_embedding = torch.cat([text_embeddings, imgs_embedding], dim=1)\n",
        "\n",
        "            return merged_embedding\n",
        "\n",
        "    def forward(self, text_enc, imgs, labels=None):\n",
        "\n",
        "        # Get the merged embeddings\n",
        "        merged_embedding = self.mvp(text_enc, imgs, self.model)\n",
        "\n",
        "        # If training include the labels\n",
        "        return self.model(inputs_embeds=merged_embedding, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymq2ts_IBoDV"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLmuZ9ibBpRD"
      },
      "outputs": [],
      "source": [
        "Config = namedtuple('Instance', ['batch_size', 'gpa_hidden_size', 'model_name', 'lm'])\n",
        "\n",
        "config = Config(\n",
        "    batch_size = 16,\n",
        "    gpa_hidden_size = 128,\n",
        "    model_name = '20240229-205610',\n",
        "    lm = 'T5-Large'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdSUOHlECNO4"
      },
      "source": [
        "## Evaluation Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTEjm2hNCPOG"
      },
      "outputs": [],
      "source": [
        "def val_model(dloader):\n",
        "\n",
        "    model.eval()\n",
        "    ids_answered = set()\n",
        "    test_data = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for idx, (q_texts, encodings, imgs, labels, img_paths) in progress_bar(enumerate(dloader), total=len(dloader)):\n",
        "\n",
        "          # Get the hidden states (output)\n",
        "          hidden_states = model(encodings, imgs, labels).logits\n",
        "\n",
        "          # Perform decoding (e.g., greedy decoding)\n",
        "          outputs = torch.argmax(hidden_states, dim=-1)\n",
        "\n",
        "          # Get the text output\n",
        "          text_outputs = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "          if idx % 100 == 0:\n",
        "            print(q_texts)\n",
        "            print(text_outputs)\n",
        "\n",
        "          for image_path, q_text, text_output in zip(img_paths, q_texts, text_outputs):\n",
        "\n",
        "              img_key = image_path[0]\n",
        "\n",
        "              # Skip duplicate questions\n",
        "              if image_id_dict[img_key + ' ' + q_text][0] in ids_answered:\n",
        "                  continue\n",
        "              if len(text_output) > config.max_len:\n",
        "                  continue\n",
        "\n",
        "              ids_answered.add(image_id_dict[img_key + ' ' + q_text][0])\n",
        "              test_data.append({'image_id': image_id_dict[img_key + ' ' + q_text][0], 'caption': text_output})\n",
        "\n",
        "    # Save test output to file\n",
        "    with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.model_name, 'predictions.json'), 'w') as f:\n",
        "        json.dump(test_data, f)\n",
        "\n",
        "\n",
        "def save_experiment():\n",
        "    \"\"\"\n",
        "    Saves the experiment results to a csv\n",
        "    :param config: The hyperparameters used\n",
        "    :param statistics: The accuracies for the training, validation, and test sets\n",
        "    \"\"\"\n",
        "\n",
        "    trial_dict = {}\n",
        "\n",
        "    # Add metrics to dictionary\n",
        "    for metric, score in coco_eval.eval.items():\n",
        "        trial_dict[metric] = [score]\n",
        "\n",
        "    trial_dict = pd.DataFrame(trial_dict)\n",
        "    trial_dict.to_csv(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.model_name, 'metrics.csv'), index=False, header=True)\n",
        "\n",
        "# Load processors and models\n",
        "model = DriveVLMT5(config)\n",
        "model.to(device)\n",
        "\n",
        "if config.lm == 'T5-Base':\n",
        "    processor = T5Tokenizer.from_pretrained('google-t5/t5-base')\n",
        "else:\n",
        "    processor = T5Tokenizer.from_pretrained('google-t5/t5-large')\n",
        "\n",
        "processor.add_tokens('<')\n",
        "\n",
        "model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.model_name,\n",
        "                                                          'latest_model.pth')))\n",
        "\n",
        "# Load dataset and dataloader\n",
        "test_dset = MultiFrameDataset(\n",
        "    input_file=os.path.join('data', 'multi_frame',\n",
        "                            'multi_frame_test.json'),\n",
        "    tokenizer=processor,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
        "    ])\n",
        ")\n",
        "test_dloader = DataLoader(test_dset, shuffle=True, batch_size=config.batch_size, drop_last=True, collate_fn=test_dset.collate_fn_test)\n",
        "\n",
        "# Load in image ids\n",
        "with open(os.path.join('drive', 'MyDrive', 'DriveLM', 'data', 'multi_frame', 'image_id.json')) as f:\n",
        "    image_id_dict = json.load(f)\n",
        "\n",
        "# Get the loss and predictions from the model\n",
        "val_model(test_dloader)\n",
        "\n",
        "annotation_file = os.path.join('drive', 'MyDrive', 'DriveLM', 'data', 'multi_frame', 'multi_frame_test_coco.json')\n",
        "results_file = os.path.join('drive', 'MyDrive', 'DriveLM', 'multi_frame_results', config.model_name, 'predictions.json')\n",
        "\n",
        "# create coco object and coco_result object\n",
        "coco = COCO(annotation_file)\n",
        "coco_result = coco.loadRes(results_file)\n",
        "\n",
        "# create coco_eval object by taking coco and coco_result\n",
        "coco_eval = COCOEvalCap(coco, coco_result)\n",
        "\n",
        "# evaluate on a subset of images by setting\n",
        "# coco_eval.params['image_id'] = coco_result.getImgIds()\n",
        "# please remove this line when evaluating the full validation set\n",
        "coco_eval.params['image_id'] = coco_result.getImgIds()\n",
        "\n",
        "# evaluate results\n",
        "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
        "coco_eval.evaluate()\n",
        "\n",
        "# Save the experiment results\n",
        "save_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPzq_32gbXUv"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgKTEiFXN7rQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPaVI48WFM0dPqIWHNZPcRB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}